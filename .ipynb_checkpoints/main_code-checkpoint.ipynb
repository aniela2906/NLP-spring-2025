{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e72945-862c-446e-872c-619dda747d5b",
   "metadata": {},
   "source": [
    "# Group 8 - main code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68efcfbe",
   "metadata": {},
   "source": [
    "### Instalations needed:  \n",
    "torch, transformers, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from transformers import RobertaTokenizerFast, AutoModelForTokenClassification, AutoConfig, DataCollatorForTokenClassification, get_scheduler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf53bee",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0811545",
   "metadata": {},
   "source": [
    "### INPUT:\n",
    " \n",
    "#### English Web Treebank sets \n",
    "- en_ewt-ud-train.iob2\n",
    "- en_ewt-ud-dev.iob2\n",
    "- en_ewt-ud-test-masked.iob2\n",
    "\n",
    "in the repository, you can find the files here:    \n",
    "/NLP-spring-2025/datasets_orginal    \n",
    "  \n",
    "  \n",
    "### OUTPUT:\n",
    "#### Baseline model:\n",
    "- baseline_model  \n",
    "recommended path:  \n",
    "  \n",
    "/NLP-spring-2025/baseline\n",
    "\n",
    "#### Prediction:  \n",
    "- baseline_predictions.iob2  \n",
    "  \n",
    "recommended path:  \n",
    "/NLP-spring-2025/baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31152907",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME, add_prefix_space=True, use_fast=True)\n",
    "\n",
    "def get_label_mappings(file_path):\n",
    "    label_set = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) > 2:\n",
    "                    label_set.add(parts[2])\n",
    "    \n",
    "    tag2idx = {label: idx for idx, label in enumerate(sorted(label_set))}\n",
    "    idx2tag = {idx: label for label, idx in tag2idx.items()}\n",
    "    return tag2idx, idx2tag\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "train_file = \"\\path\\en_ewt-ud-train.iob2\"             #path to EWT training set \n",
    "dev_file = \"\\path\\en_ewt-ud-dev.iob2\"                 #path to EWT dev set \n",
    "test_file = \"\\prth\\en_ewt-ud-test-masked.iob2\"        #path to EWT test set \n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "tag2idx, idx2tag = get_label_mappings(train_file)\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, tag2idx, max_len=128):\n",
    "        self.sentences, self.labels, self.raw_data = self.load_data(file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, labels, raw_data = [], [], []\n",
    "        sentence, label, sentence_data = [], [], []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    sentence_data.append(line)\n",
    "                    if line.startswith('#'):\n",
    "                        continue\n",
    "                    \n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) > 2:\n",
    "                        sentence.append(parts[1])  # Word\n",
    "                        label.append(parts[2])  # NER tag\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        labels.append(label)\n",
    "                        raw_data.append(sentence_data)\n",
    "                    sentence, label, sentence_data = [], [], []\n",
    "        \n",
    "        if sentence: \n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "            raw_data.append(sentence_data)\n",
    "            \n",
    "        return sentences, labels, raw_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.sentences[idx]\n",
    "        tags = self.labels[idx]\n",
    "        \n",
    "        tag_ids = [self.tag2idx[tag] for tag in tags]\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=False\n",
    "        )\n",
    "        \n",
    "        word_ids = encodings.word_ids()\n",
    "        \n",
    "        aligned_labels = [-100] * len(word_ids)\n",
    "        \n",
    "        prev_word_id = None\n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                # Special tokens get -100\n",
    "                aligned_labels[i] = -100\n",
    "            elif word_id != prev_word_id:\n",
    "                # Only first subword of a given word gets the label\n",
    "                aligned_labels[i] = tag_ids[word_id]\n",
    "            else:\n",
    "                # Other subwords get -100\n",
    "                aligned_labels[i] = -100\n",
    "            prev_word_id = word_id\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(encodings['input_ids']),\n",
    "            'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "            'labels': torch.tensor(aligned_labels)\n",
    "        }\n",
    "\n",
    "train_dataset = NERDataset(train_file, tokenizer, tag2idx)\n",
    "dev_dataset = NERDataset(dev_file, tokenizer, tag2idx)\n",
    "test_dataset = NERDataset(test_file, tokenizer, tag2idx)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "full_train_dataset = ConcatDataset([train_dataset, dev_dataset])\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(tag2idx),\n",
    "    id2label=idx2tag,\n",
    "    label2id=tag2idx\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train_batch_size = 16\n",
    "eval_batch_size = 32\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "epochs = 5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    full_train_dataset, \n",
    "    batch_size=train_batch_size, \n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset, \n",
    "    batch_size=eval_batch_size, \n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=eval_batch_size, \n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "try:\n",
    "    metric = load(\"seqeval\")\n",
    "except:\n",
    "    metric = None\n",
    "    print(\"Warning: 'evaluate' package not found. Will skip detailed metrics calculation.\")\n",
    "\n",
    "def pred2label(predictions, labels):\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for pred_seq, label_seq in zip(preds, labels):\n",
    "        true_seq = []\n",
    "        pred_seq_clean = []\n",
    "        \n",
    "        for i, label_id in enumerate(label_seq):\n",
    "            if label_id != -100:\n",
    "                true_seq.append(idx2tag[label_id.item()])\n",
    "                pred_seq_clean.append(idx2tag[pred_seq[i]])\n",
    "        \n",
    "        true_labels.append(true_seq)\n",
    "        pred_labels.append(pred_seq_clean)\n",
    "    \n",
    "    return true_labels, pred_labels\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            predictions = logits.detach().cpu().numpy()\n",
    "            labels = batch[\"labels\"].detach().cpu().numpy()\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    all_predictions = np.vstack([p.reshape(-1, len(tag2idx)) for p in all_predictions])\n",
    "    all_labels = np.concatenate([l.flatten() for l in all_labels])\n",
    "    \n",
    "    valid_indices = all_labels != -100\n",
    "    filtered_predictions = all_predictions[valid_indices]\n",
    "    filtered_labels = all_labels[valid_indices]\n",
    "    \n",
    "    accuracy = np.mean(np.argmax(filtered_predictions, axis=1) == filtered_labels)\n",
    "    \n",
    "    return {\n",
    "        \"loss\": total_loss / len(dataloader),\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "scaler = GradScaler()\n",
    "best_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(f\"Starting training on {device}\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    eval_results = evaluate(model, dev_loader, device)\n",
    "    val_losses.append(eval_results[\"loss\"])\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train loss: {avg_train_loss:.4f} - Val loss: {eval_results['loss']:.4f} - Val accuracy: {eval_results['accuracy']:.4f}\")\n",
    "    \n",
    "    if eval_results[\"loss\"] < best_loss:\n",
    "        best_loss = eval_results[\"loss\"]\n",
    "        model.save_pretrained(\"baseline_model\")\n",
    "        tokenizer.save_pretrained(\"baseline_model\")\n",
    "        print(\"Model saved!\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"training_curve.png\")\n",
    "plt.close()\n",
    "\n",
    "def write_predictions(model, dataset, dataloader, output_path):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating predictions\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "            predictions = outputs.logits.argmax(dim=-1).detach().cpu().numpy()\n",
    "            labels = batch[\"labels\"].detach().cpu().numpy()\n",
    "            \n",
    "            for i in range(len(predictions)):\n",
    "                pred_tags = []\n",
    "                for j, pred in enumerate(predictions[i]):\n",
    "                    if labels[i][j] != -100:\n",
    "                        pred_tags.append(idx2tag[pred.item()])\n",
    "                \n",
    "                all_predictions.append(pred_tags)\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        pred_idx = 0\n",
    "        for i, data in enumerate(dataset.raw_data):\n",
    "            sentence_counter = 0\n",
    "            for line in data:\n",
    "                if line.startswith('#'):\n",
    "                    f.write(f\"{line}\\n\")\n",
    "                    continue\n",
    "                \n",
    "                if line:\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) > 2:\n",
    "                        if sentence_counter < len(all_predictions[pred_idx]):\n",
    "                            f.write(f\"{parts[0]}\\t{parts[1]}\\t{all_predictions[pred_idx][sentence_counter]}\\t-\\t-\\n\")\n",
    "                            sentence_counter += 1\n",
    "                        else:\n",
    "                            f.write(f\"{parts[0]}\\t{parts[1]}\\tO\\t-\\t-\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "            pred_idx += 1\n",
    "    \n",
    "    print(f\"Predictions saved to: {output_path}\")\n",
    "\n",
    "best_model = AutoModelForTokenClassification.from_pretrained(\"baseline_model\")\n",
    "best_model.to(device)\n",
    "write_predictions(best_model, test_dataset, test_loader, \"baseline_predictions.iob2\")\n",
    "\n",
    "print(\"Training and evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15482107",
   "metadata": {},
   "source": [
    "___________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7637360f",
   "metadata": {},
   "source": [
    "For the next part you will be using DAPT NER model - model after domain-adaptive pretraining.  \n",
    "In README github there is instruction for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e675521-1101-4de5-bfaa-b68a0290a214",
   "metadata": {},
   "source": [
    "# Fine-turning the  DAPT  NER model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598bf2a",
   "metadata": {},
   "source": [
    "### INPUT:\n",
    " \n",
    " #### NER model with domain adaptive pretraining\n",
    "- ner_DAPT_model \n",
    "\n",
    "\n",
    "#### English Web Treebank training set \n",
    "- en_ewt-ud-train.iob2  \n",
    "\n",
    "in the repository, you can find the files here:    \n",
    "/NLP-spring-2025/datasets_orginal    \n",
    "  \n",
    "  \n",
    "#### training dataset for fine-tuning (one of):\n",
    "\n",
    "- rap-hip-hop-manual-1000-train.iob2   \n",
    "- pop-manual-1000-train.iob2   \n",
    "- country-manual-1000-train.iob2   \n",
    "- 3genres_.iob2   \n",
    " \n",
    "in the repository, you can find the files here:  \n",
    "/NLP-spring-2025/'GENRE'/datasets/manual  \n",
    "  \n",
    " ### OUTPUT:\n",
    " #### Fine-tuned new model (one of):\n",
    "\n",
    "- ner_DAPT_model_finetuned_on_3genres   \n",
    "- ner_DAPT_model_finetuned_on_rap-hip-hop   \n",
    "- ner_DAPT_model_finetuned_on_pop   \n",
    "- ner_DAPT_model_finetuned_on_country   \n",
    "  \n",
    "recommended path:  \n",
    "/NLP-spring-2025/'GENRE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c5207-7da7-402f-8006-8c4144153b25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T10:15:16.016521Z",
     "iopub.status.busy": "2025-04-30T10:15:16.015564Z",
     "iopub.status.idle": "2025-04-30T10:22:19.895049Z",
     "shell.execute_reply": "2025-04-30T10:22:19.894164Z",
     "shell.execute_reply.started": "2025-04-30T10:15:16.016461Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------\n",
    "BASE_MODEL_PATH = \"/path/ner_DAPT_model\"                         # path to DAPT NER model \n",
    "NEW_TRAIN_FILE = \"/path/pop-manual-1000-train.iob2\"              # path to training dataset \n",
    "OUTPUT_MODEL_PATH = \"/path/ner_DAPT_model_finetuned_on_pop\"      # path to output model (DOMAIN SPECIFIC FINE-TUNING)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "epochs = 5\n",
    "train_batch_size = 16\n",
    "learning_rate = 5e-6  \n",
    "max_len = 128\n",
    "\n",
    "\n",
    "#load tokenizer and model\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(BASE_MODEL_PATH, add_prefix_space=True, use_fast=True)\n",
    "\n",
    "#reuse label mappings from EWT\n",
    "def get_label_mappings(file_path):\n",
    "    label_set = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) > 2:\n",
    "                    label_set.add(parts[2])\n",
    "    tag2idx = {label: idx for idx, label in enumerate(sorted(label_set))}\n",
    "    idx2tag = {idx: label for label, idx in tag2idx.items()}\n",
    "    return tag2idx, idx2tag\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "tag2idx, idx2tag = get_label_mappings(\"/path/en_ewt-ud-train.iob2\")  #path to ewt training data\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, tag2idx, max_len=128):\n",
    "        self.sentences, self.labels = self.load_data(file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, labels = [], []\n",
    "        sentence, label = [], []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    if not line.startswith('#'):\n",
    "                        parts = line.split('\\t')\n",
    "                        if len(parts) > 2:\n",
    "                            sentence.append(parts[1])\n",
    "                            label.append(parts[2])\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        labels.append(label)\n",
    "                        sentence, label = [], []\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "        return sentences, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.sentences[idx]\n",
    "        tags = self.labels[idx]\n",
    "        tag_ids = [self.tag2idx[tag] for tag in tags]\n",
    "\n",
    "        encodings = self.tokenizer(words, is_split_into_words=True, truncation=True, max_length=self.max_len, padding=False)\n",
    "        word_ids = encodings.word_ids()\n",
    "\n",
    "        aligned_labels = [-100] * len(word_ids)\n",
    "        prev_word_id = None\n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                aligned_labels[i] = -100\n",
    "            elif word_id != prev_word_id:\n",
    "                aligned_labels[i] = tag_ids[word_id]\n",
    "            else:\n",
    "                aligned_labels[i] = -100\n",
    "            prev_word_id = word_id\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encodings['input_ids']),\n",
    "            'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "            'labels': torch.tensor(aligned_labels)\n",
    "        }\n",
    "\n",
    "#load training dataset\n",
    "lyrics_dataset = NERDataset(NEW_TRAIN_FILE, tokenizer, tag2idx)\n",
    "lyrics_loader = DataLoader(lyrics_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=DataCollatorForTokenClassification(tokenizer))\n",
    "\n",
    "#load model\n",
    "config = AutoConfig.from_pretrained(BASE_MODEL_PATH, num_labels=len(tag2idx), id2label=idx2tag, label2id=tag2idx)\n",
    "model = AutoModelForTokenClassification.from_pretrained(BASE_MODEL_PATH, config=config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "num_training_steps = len(lyrics_loader) * epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0.1*num_training_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "#fine-tuning\n",
    "print(\"Starting fine-tuning\")\n",
    "model.train()\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(lyrics_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with autocast():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(lyrics_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1} completed. Average loss: {avg_loss:.4f}\")\n",
    "\n",
    "#save the new fine-tuned model\n",
    "os.makedirs(OUTPUT_MODEL_PATH, exist_ok=True)\n",
    "model.save_pretrained(OUTPUT_MODEL_PATH)\n",
    "tokenizer.save_pretrained(OUTPUT_MODEL_PATH)\n",
    "print(f\"New fine-tuned model saved to {OUTPUT_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0590c804-1ba2-4927-bb6e-bdfc90809a79",
   "metadata": {},
   "source": [
    "# Generating pseudo-labels on unlabeled data using a previously fine-tuned model, as part of a self-training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a120461",
   "metadata": {},
   "source": [
    "## CODE 1 (for a single genre)\n",
    "### INPUT:\n",
    "#### ner_DAPT_model fine-tuned  (one of):  \n",
    "\n",
    "- ner_DAPT_model_finetuned_on_rap-hip-hop     \n",
    "- ner_DAPT_model_finetuned_on_pop     \n",
    "- ner_DAPT_model_finetuned_on_country   \n",
    "  \n",
    "if you follow the recommended paths:  \n",
    "/NLP-spring-2025/'GENRE' \n",
    "    \n",
    "#### dataset for generating more training datasets  (one of):\n",
    "\n",
    "- rap-hip-hop_labeled_no_2000_all_O.iob2  \n",
    "- pop_labeled_no_2000_all_O.iob2  \n",
    "- country_labeled_no_2000_all_O.iob2  \n",
    "\n",
    "in the repository, you can find the files here:    \n",
    "/NLP-spring-2025/'GENRE'/datasets  \n",
    "  \n",
    "### OUTPUT:\n",
    "#### new training dataset (one of):  \n",
    " \n",
    "\n",
    "- rap-hip-hop_labeled_no_2000.iob2\n",
    "- pop_labeled_no_2000.iob2\n",
    "- country_labeled_no_2000.iob2\n",
    "\n",
    "recommended path:  \n",
    "/NLP-spring-2025/'GENRE'/datasets \n",
    " \n",
    "-----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## CODE 2 (3 genres combined) \n",
    "### INPUT:\n",
    "#### ner_DAPT_model fine-tuned :\n",
    "\n",
    "- ner_DAPT_model_finetuned_on_3genres \n",
    "  \n",
    "if you follow the recommended paths:    \n",
    "/NLP-spring-2025/3genres   \n",
    "    \n",
    "####  datasets for generating more training datasets (all): \n",
    "\n",
    "- rap-hip-hop_labeled_no_2000_all_O.iob2  \n",
    "- pop_labeled_no_2000_all_O.iob2  \n",
    "- country_labeled_no_2000_all_O.iob2  \n",
    "\n",
    "in the repository, you can find the files here:      \n",
    "/NLP-spring-2025/'GENRE'/datasets  \n",
    " \n",
    "### OUTPUT:\n",
    "#### new training dataset:\n",
    "\n",
    "- merged_3genres_labeled.iob2\n",
    " \n",
    "recommended path:    \n",
    "/NLP-spring-2025/3genres/datasets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a442590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------\n",
    "#\n",
    "#\n",
    "#                                            CODE 1 \n",
    "#\n",
    "#\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "MODEL_PATH = \"/path/ner_DAPT_model_finetuned_on_pop\"   #path to the fine-tuned model \n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_PATH, add_prefix_space=True, use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#label mappings \n",
    "tag2idx = model.config.label2id\n",
    "idx2tag = model.config.id2label\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, tag2idx, max_len=128):\n",
    "        self.sentences, self.labels, self.raw_data = self.load_data(file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, labels, raw_data = [], [], []\n",
    "        sentence, label, sentence_data = [], [], []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    sentence_data.append(line)\n",
    "                    if line.startswith('#'):\n",
    "                        continue\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) > 2:\n",
    "                        sentence.append(parts[1])\n",
    "                        label.append(parts[2])\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        labels.append(label)\n",
    "                        raw_data.append(sentence_data)\n",
    "                    sentence, label, sentence_data = [], [], []\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "            raw_data.append(sentence_data)\n",
    "        return sentences, labels, raw_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.sentences[idx]\n",
    "        tags = self.labels[idx]\n",
    "        tag_ids = [self.tag2idx.get(tag, 0) for tag in tags]\n",
    "        encodings = self.tokenizer(\n",
    "            words, is_split_into_words=True, truncation=True,\n",
    "            max_length=self.max_len, padding=False\n",
    "        )\n",
    "        word_ids = encodings.word_ids()\n",
    "        aligned_labels = [-100] * len(word_ids)\n",
    "        prev_word_id = None\n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                aligned_labels[i] = -100\n",
    "            elif word_id != prev_word_id:\n",
    "                aligned_labels[i] = tag_ids[word_id]\n",
    "            else:\n",
    "                aligned_labels[i] = -100\n",
    "            prev_word_id = word_id\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encodings['input_ids']),\n",
    "            'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "            'labels': torch.tensor(aligned_labels)\n",
    "        }\n",
    "\n",
    "#prediction function\n",
    "def write_predictions(model, dataset, dataloader, output_path):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "            for i in range(len(predictions)):\n",
    "                pred_tags = []\n",
    "                for j, pred in enumerate(predictions[i]):\n",
    "                    if labels[i][j] != -100:\n",
    "                        pred_tags.append(idx2tag[pred.item()])\n",
    "                all_predictions.append(pred_tags)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        pred_idx = 0\n",
    "        for data in dataset.raw_data:\n",
    "            sent_counter = 0\n",
    "            for line in data:\n",
    "                if line.startswith('#'):\n",
    "                    f.write(f\"{line}\\n\")\n",
    "                    continue\n",
    "                if line:\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) > 2:\n",
    "                        tag = all_predictions[pred_idx][sent_counter] if sent_counter < len(all_predictions[pred_idx]) else \"O\"\n",
    "                        f.write(f\"{parts[0]}\\t{parts[1]}\\t{tag}\\t-\\t-\\n\")\n",
    "                        sent_counter += 1\n",
    "            f.write(\"\\n\")\n",
    "            pred_idx += 1\n",
    "    print(f\"Predictions written to {output_path}\")\n",
    "\n",
    "\n",
    "    \n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "test_file = \"/path/pop_labeled_no_2000_all_O.iob2\"  #path to not labeled training file\n",
    "output_file = \"/path/pop_labeled_no_2000.iob2\"      #path to output file (new training file)\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "test_dataset = NERDataset(test_file, tokenizer, tag2idx)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=DataCollatorForTokenClassification(tokenizer))\n",
    "\n",
    "write_predictions(model, test_dataset, test_loader, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7687e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------\n",
    "#\n",
    "#\n",
    "#                                            CODE 2 \n",
    "#\n",
    "#\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "MODEL_PATH = \"/work/project/ner_model_finetuned_on_3genres\"   #path to the fine-tuned model \n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_PATH, add_prefix_space=True, use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#label mappings\n",
    "tag2idx = model.config.label2id\n",
    "idx2tag = model.config.id2label\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, tag2idx, max_len=128):\n",
    "        self.sentences, self.labels, self.raw_data = self.load_data(file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, labels, raw_data = [], [], []\n",
    "        sentence, label, sentence_data = [], [], []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    sentence_data.append(line)\n",
    "                    if line.startswith('#'):\n",
    "                        continue\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) > 2:\n",
    "                        sentence.append(parts[1])\n",
    "                        label.append(parts[2])\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        labels.append(label)\n",
    "                        raw_data.append(sentence_data)\n",
    "                    sentence, label, sentence_data = [], [], []\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "            raw_data.append(sentence_data)\n",
    "        return sentences, labels, raw_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.sentences[idx]\n",
    "        tags = self.labels[idx]\n",
    "        tag_ids = [self.tag2idx.get(tag, 0) for tag in tags]\n",
    "        encodings = self.tokenizer(\n",
    "            words, is_split_into_words=True, truncation=True,\n",
    "            max_length=self.max_len, padding=False\n",
    "        )\n",
    "        word_ids = encodings.word_ids()\n",
    "        aligned_labels = [-100] * len(word_ids)\n",
    "        prev_word_id = None\n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                aligned_labels[i] = -100\n",
    "            elif word_id != prev_word_id:\n",
    "                aligned_labels[i] = tag_ids[word_id]\n",
    "            else:\n",
    "                aligned_labels[i] = -100\n",
    "            prev_word_id = word_id\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encodings['input_ids']),\n",
    "            'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "            'labels': torch.tensor(aligned_labels)\n",
    "        }\n",
    "\n",
    "#prediction function\n",
    "def write_predictions(model, dataset, dataloader, output_path):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "            for i in range(len(predictions)):\n",
    "                pred_tags = []\n",
    "                for j, pred in enumerate(predictions[i]):\n",
    "                    if labels[i][j] != -100:\n",
    "                        pred_tags.append(idx2tag[pred.item()])\n",
    "                all_predictions.append(pred_tags)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        pred_idx = 0\n",
    "        for data in dataset.raw_data:\n",
    "            sent_counter = 0\n",
    "            for line in data:\n",
    "                if line.startswith('#'):\n",
    "                    f.write(f\"{line}\\n\")\n",
    "                    continue\n",
    "                if line:\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) > 2:\n",
    "                        tag = all_predictions[pred_idx][sent_counter] if sent_counter < len(all_predictions[pred_idx]) else \"O\"\n",
    "                        f.write(f\"{parts[0]}\\t{parts[1]}\\t{tag}\\t-\\t-\\n\")\n",
    "                        sent_counter += 1\n",
    "            f.write(\"\\n\")\n",
    "            pred_idx += 1\n",
    "    print(f\"Predictions written to {output_path}\")\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "test_files = [\n",
    "    \"/path/pop_labeled_no_2000_all_O.iob2\",  \n",
    "    \"path/country_labeled_no_2000_all_O.iob2\",\n",
    "    \"/path/rap-hip-hop/rap-hip-hop_labeled_no_2000_all_O.iob2\"     #paths to all not labeled training files\n",
    "]\n",
    "\n",
    "\n",
    "output_file = \"/path/merged_3genres_labeled.iob2\"              #path to output file (new training file)\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for test_file in test_files:\n",
    "        print(f\" Processing {test_file}\")\n",
    "\n",
    "        #load test dataset\n",
    "        test_dataset = NERDataset(test_file, tokenizer, tag2idx)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=DataCollatorForTokenClassification(tokenizer))\n",
    "\n",
    "        #predict\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=f\"Predicting {os.path.basename(test_file)}\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                predictions = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "                labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "                for i in range(len(predictions)):\n",
    "                    pred_tags = []\n",
    "                    for j, pred in enumerate(predictions[i]):\n",
    "                        if labels[i][j] != -100:\n",
    "                            pred_tags.append(idx2tag[pred.item()])\n",
    "                    all_predictions.append(pred_tags)\n",
    "\n",
    "        #write predictions\n",
    "        pred_idx = 0\n",
    "        for data in test_dataset.raw_data:\n",
    "            sent_counter = 0\n",
    "            for line in data:\n",
    "                if line.startswith('#'):\n",
    "                    fout.write(f\"{line}\\n\")\n",
    "                    continue\n",
    "                if line:\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) > 2:\n",
    "                        tag = all_predictions[pred_idx][sent_counter] if sent_counter < len(all_predictions[pred_idx]) else \"O\"\n",
    "                        fout.write(f\"{parts[0]}\\t{parts[1]}\\t{tag}\\t-\\t-\\n\")\n",
    "                        sent_counter += 1\n",
    "            fout.write(\"\\n\")\n",
    "            pred_idx += 1\n",
    "\n",
    "print(f\" All predictions written to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf3cb4-99b8-47d7-9829-bd576b9fc0c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T14:51:49.525133Z",
     "iopub.status.busy": "2025-04-29T14:51:49.524285Z",
     "iopub.status.idle": "2025-04-29T14:51:49.586159Z",
     "shell.execute_reply": "2025-04-29T14:51:49.585290Z",
     "shell.execute_reply.started": "2025-04-29T14:51:49.525075Z"
    }
   },
   "source": [
    "# Continuous Learning (Self-Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d11e2",
   "metadata": {},
   "source": [
    "### INPUT:\n",
    "#### ner_DAPT_model fine-tuned  (one of):      \n",
    "  \n",
    "- ner_DAPT_model_finetuned_on_rap-hip-hop     \n",
    "- ner_DAPT_model_finetuned_on_pop     \n",
    "- ner_DAPT_model_finetuned_on_country     \n",
    "- ner_DAPT_model_finetuned_on_3genres  \n",
    "  \n",
    "if you follow the recommended paths:      \n",
    "/NLP-spring-2025/'GENRE'\n",
    "\n",
    "#### training dataset for continuous learning (one of):  \n",
    "\n",
    "- rap-hip-hop_labeled_no_2000.iob2\n",
    "- pop_labeled_no_2000.iob2\n",
    "- country_labeled_no_2000.iob2\n",
    "- merged_3genres_labeled.iob2  \n",
    "  \n",
    "if you follow the recomended paths:         \n",
    "/NLP-spring-2025/'GENRE'/datasets \n",
    "\n",
    "\n",
    "#### English Web Treebank training set \n",
    "- en_ewt-ud-train.iob2  \n",
    "  \n",
    "in the repository, you can find the files here:  \n",
    "/NLP-spring-2025/datasets_orginal   \n",
    "  \n",
    "### OUTPUT:\n",
    "#### new ner DAPT model - after continuous learning (one of):  \n",
    "\n",
    "- ner_DAPT_model_cont_on_rap-hip-hop\n",
    "- ner_DAPT_model_cont_on_pop\n",
    "- ner_DAPT_model_cont_on_country\n",
    "- ner_DAPT_model_cont_on_3genres  \n",
    "  \n",
    "recommended path:    \n",
    "/NLP-spring-2025/'GENRE' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be26fb-9d0d-4d13-89b8-803f49372a8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T11:04:15.774984Z",
     "iopub.status.busy": "2025-04-30T11:04:15.774252Z",
     "iopub.status.idle": "2025-04-30T12:51:41.261199Z",
     "shell.execute_reply": "2025-04-30T12:51:41.260320Z",
     "shell.execute_reply.started": "2025-04-30T11:04:15.774914Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "BASE_MODEL_PATH = \"/path/ner_DAPT_model_finetuned_on_pop\"         # path to DAPT NER model\n",
    "NEW_TRAIN_FILE = \"/path/pop_labeled_no_2000.iob2\"            #path to new training dataset\n",
    "OUTPUT_MODEL_PATH = \"/path/ner_DAPT_model_cont_on_pop\"       #new model (CONTINUOUS LEARNING)\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "epochs = 3\n",
    "train_batch_size = 64\n",
    "learning_rate = 5e-6  \n",
    "max_len = 128\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(BASE_MODEL_PATH, add_prefix_space=True, use_fast=True)\n",
    "\n",
    "#reuse label mappings from EWT \n",
    "def get_label_mappings(file_path):\n",
    "    label_set = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) > 2:\n",
    "                    label_set.add(parts[2])\n",
    "    tag2idx = {label: idx for idx, label in enumerate(sorted(label_set))}\n",
    "    idx2tag = {idx: label for label, idx in tag2idx.items()}\n",
    "    return tag2idx, idx2tag\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "tag2idx, idx2tag = get_label_mappings(\"/path/en_ewt-ud-train.iob2\") #path to ewt training data\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "# Dataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, tag2idx, max_len=128):\n",
    "        self.sentences, self.labels = self.load_data(file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, labels = [], []\n",
    "        sentence, label = [], []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    if not line.startswith('#'):\n",
    "                        parts = line.split('\\t')\n",
    "                        if len(parts) > 2:\n",
    "                            sentence.append(parts[1])\n",
    "                            label.append(parts[2])\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        labels.append(label)\n",
    "                        sentence, label = [], []\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "        return sentences, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.sentences[idx]\n",
    "        tags = self.labels[idx]\n",
    "        tag_ids = [self.tag2idx[tag] for tag in tags]\n",
    "\n",
    "        encodings = self.tokenizer(words, is_split_into_words=True, truncation=True, max_length=self.max_len, padding=False)\n",
    "        word_ids = encodings.word_ids()\n",
    "\n",
    "        aligned_labels = [-100] * len(word_ids)\n",
    "        prev_word_id = None\n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                aligned_labels[i] = -100\n",
    "            elif word_id != prev_word_id:\n",
    "                aligned_labels[i] = tag_ids[word_id]\n",
    "            else:\n",
    "                aligned_labels[i] = -100\n",
    "            prev_word_id = word_id\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encodings['input_ids']),\n",
    "            'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "            'labels': torch.tensor(aligned_labels)\n",
    "        }\n",
    "\n",
    "#load new dataset\n",
    "lyrics_dataset = NERDataset(NEW_TRAIN_FILE, tokenizer, tag2idx)\n",
    "lyrics_loader = DataLoader(lyrics_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=DataCollatorForTokenClassification(tokenizer))\n",
    "\n",
    "#load model\n",
    "config = AutoConfig.from_pretrained(BASE_MODEL_PATH, num_labels=len(tag2idx), id2label=idx2tag, label2id=tag2idx)\n",
    "model = AutoModelForTokenClassification.from_pretrained(BASE_MODEL_PATH, config=config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "num_training_steps = len(lyrics_loader) * epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0.1*num_training_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "#continuous learning\n",
    "print(\"Starting continuous learning:\")\n",
    "model.train()\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(lyrics_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with autocast():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(lyrics_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1} completed. Average loss: {avg_loss:.4f}\")\n",
    "\n",
    "#save the new model after continuous learning\n",
    "os.makedirs(OUTPUT_MODEL_PATH, exist_ok=True)\n",
    "model.save_pretrained(OUTPUT_MODEL_PATH)\n",
    "tokenizer.save_pretrained(OUTPUT_MODEL_PATH)\n",
    "print(f\"New model (after continuous learning) saved to {OUTPUT_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54128624-058d-4362-9b52-c61c5a92d963",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f88b33",
   "metadata": {},
   "source": [
    "### INPUT:\n",
    "#### ner_DAPT_model path (one of):  \n",
    "- ner_DAPT_model_finetuned_on_rap-hip-hop     \n",
    "- ner_DAPT_model_finetuned_on_pop     \n",
    "- ner_DAPT_model_finetuned_on_country     \n",
    "- ner_DAPT_model_finetuned_on_3genres\n",
    "\n",
    "- ner_DAPT_model_cont_on_rap-hip-hop\n",
    "- ner_DAPT_model_cont_on_pop\n",
    "- ner_DAPT_model_cont_on_country\n",
    "- ner_DAPT_model_cont_on_3genres  \n",
    "    \n",
    "if you follow the recomended paths:  \n",
    "/NLP-spring-2025/'GENRE'  \n",
    "\n",
    "#### training dataset for continuous learning (one of):\n",
    "\n",
    "- rap-hip-hop_labeled_no_2000.iob2\n",
    "- pop_labeled_no_2000.iob2\n",
    "- country_labeled_no_2000.iob2\n",
    "- merged_3genres_labeled.iob2  \n",
    "  \n",
    "if you follow the recommended paths:  \n",
    "/NLP-spring-2025/'GENRE'/datasets    \n",
    "  \n",
    "### OUTPUT:\n",
    "#### prediction file (one of):  \n",
    "- predictions_rap-hip-hop.iob2\n",
    "- predictions_pop.iob2\n",
    "- predictions_country.iob2\n",
    "- predictions_3genres.iob2\n",
    "\n",
    "- predictions_continuous_learning_rap-hip-hop.iob2\n",
    "- predictions_continuous_learning_pop.iob2\n",
    "- predictions_continuous_learning_country.iob2\n",
    "- predictions_continuous_learning_3genres.iob2\n",
    "\n",
    "recommended path:  \n",
    "/NLP-spring-2025/'GENRE'/predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "MODEL_PATH = \"/path/ner_DAPT_model_finetuned_on_pop\"\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_PATH, add_prefix_space=True, use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#label mappings\n",
    "tag2idx = model.config.label2id\n",
    "idx2tag = model.config.id2label\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, tag2idx, max_len=128):\n",
    "        self.sentences, self.labels, self.raw_data = self.load_data(file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        sentences, labels, raw_data = [], [], []\n",
    "        sentence, label, sentence_data = [], [], []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    sentence_data.append(line)\n",
    "                    if line.startswith('#'):\n",
    "                        continue\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) > 2:\n",
    "                        sentence.append(parts[1])\n",
    "                        label.append(parts[2])\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        labels.append(label)\n",
    "                        raw_data.append(sentence_data)\n",
    "                    sentence, label, sentence_data = [], [], []\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "            raw_data.append(sentence_data)\n",
    "        return sentences, labels, raw_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.sentences[idx]\n",
    "        tags = self.labels[idx]\n",
    "        tag_ids = [self.tag2idx.get(tag, 0) for tag in tags]\n",
    "        encodings = self.tokenizer(\n",
    "            words, is_split_into_words=True, truncation=True,\n",
    "            max_length=self.max_len, padding=False\n",
    "        )\n",
    "        word_ids = encodings.word_ids()\n",
    "        aligned_labels = [-100] * len(word_ids)\n",
    "        prev_word_id = None\n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                aligned_labels[i] = -100\n",
    "            elif word_id != prev_word_id:\n",
    "                aligned_labels[i] = tag_ids[word_id]\n",
    "            else:\n",
    "                aligned_labels[i] = -100\n",
    "            prev_word_id = word_id\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encodings['input_ids']),\n",
    "            'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "            'labels': torch.tensor(aligned_labels)\n",
    "        }\n",
    "\n",
    "#prediction function\n",
    "def write_predictions(model, dataset, dataloader, output_path):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "            for i in range(len(predictions)):\n",
    "                pred_tags = []\n",
    "                for j, pred in enumerate(predictions[i]):\n",
    "                    if labels[i][j] != -100:\n",
    "                        pred_tags.append(idx2tag[pred.item()])\n",
    "                all_predictions.append(pred_tags)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        pred_idx = 0\n",
    "        for data in dataset.raw_data:\n",
    "            sent_counter = 0\n",
    "            for line in data:\n",
    "                if line.startswith('#'):\n",
    "                    f.write(f\"{line}\\n\")\n",
    "                    continue\n",
    "                if line:\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) > 2:\n",
    "                        tag = all_predictions[pred_idx][sent_counter] if sent_counter < len(all_predictions[pred_idx]) else \"O\"\n",
    "                        f.write(f\"{parts[0]}\\t{parts[1]}\\t{tag}\\t-\\t-\\n\")\n",
    "                        sent_counter += 1\n",
    "            f.write(\"\\n\")\n",
    "            pred_idx += 1\n",
    "    print(f\"Predictions written to {output_path}\")\n",
    "\n",
    "#run prediction\n",
    "test_file = \"/path/lyrics_test_no_labels.iob2\"\n",
    "output_file = \"/path/predictions_pop.iob2\"\n",
    "\n",
    "test_dataset = NERDataset(test_file, tokenizer, tag2idx)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=DataCollatorForTokenClassification(tokenizer))\n",
    "\n",
    "write_predictions(model, test_dataset, test_loader, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee4d98-b397-4f3d-b613-37766ac6da96",
   "metadata": {},
   "source": [
    "# Statistics\n",
    " #### Input:\n",
    " \n",
    "prediction file (one of):  \n",
    "\n",
    "   - predictions_rap-hip-hop.iob2\n",
    "\n",
    "   - predictions_pop.iob2\n",
    "\n",
    "   - predictions_country.iob2\n",
    "\n",
    "   - predictions_3genres.iob2\n",
    "\n",
    "   - predictions_continuous_learning_rap-hip-hop.iob2\n",
    "\n",
    "   - predictions_continuous_learning_pop.iob2\n",
    "\n",
    "   - predictions_continuous_learning_country.iob2\n",
    "\n",
    "   - predictions_continuous_learning_3genres.iob2\n",
    "   \n",
    "if you follow the recommended paths:  \n",
    "/NLP-spring-2025/'GENRE'/predictions  \n",
    "  \n",
    "and  \n",
    "  \n",
    "   - golden file ( lyrics_test.iob2 )\n",
    "     \n",
    "in the repository, you can find the files here:  \n",
    "/NLP-spring-2025/test \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Mistake breakdown:\n",
    "#### correct   \n",
    "Model correctly predicted the label (e.g. B-PER → B-PER)    \n",
    "\n",
    "#### wrong_label    \n",
    "Model predicted a named entity, but with the wrong type or boundary (e.g. B-LOC → B-ORG).     \n",
    "       THIS FOLLOWS UP WITH A DETAILED BREAKDOWN       \n",
    "  \n",
    "#### spurious    \n",
    "Model predicted an entity where the gold label was O. These are false positives — overpredictions.  \n",
    "\n",
    "#### missed    \n",
    "Model predicted O where the gold label was a named entity. These are false negatives — missed detections.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68c198-4102-4bf9-9fa7-61a9dcf93c9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:05:15.712993Z",
     "iopub.status.busy": "2025-04-30T13:05:15.712260Z",
     "iopub.status.idle": "2025-04-30T13:05:15.775172Z",
     "shell.execute_reply": "2025-04-30T13:05:15.774452Z",
     "shell.execute_reply.started": "2025-04-30T13:05:15.712933Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_errors(gold_file, pred_file):\n",
    "    def read_labels(path):\n",
    "        sentences = []\n",
    "        current = []\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == \"\":\n",
    "                    if current:\n",
    "                        sentences.append(current)\n",
    "                        current = []\n",
    "                elif line.startswith(\"#\"):\n",
    "                    continue\n",
    "                else:\n",
    "                    current.append(line.split(\"\\t\")[2])  # third column\n",
    "        if current:\n",
    "            sentences.append(current)\n",
    "        return sentences\n",
    "\n",
    "    gold = read_labels(gold_file)\n",
    "    pred = read_labels(pred_file)\n",
    "\n",
    "    assert len(gold) == len(pred), \"Mismatch in number of sentences\"\n",
    "\n",
    "    counter = Counter()\n",
    "\n",
    "    for g_sent, p_sent in zip(gold, pred):\n",
    "        assert len(g_sent) == len(p_sent), \"Mismatch in sentence lengths\"\n",
    "        for g, p in zip(g_sent, p_sent):\n",
    "            if g == p:\n",
    "                counter[\"correct\"] += 1\n",
    "            elif g == \"O\" and p != \"O\":\n",
    "                counter[\"spurious\"] += 1\n",
    "            elif g != \"O\" and p == \"O\":\n",
    "                counter[\"missed\"] += 1\n",
    "            elif g != p:\n",
    "                counter[f\"{g}->{p}\"] += 1\n",
    "                counter[\"wrong_label\"] += 1\n",
    "\n",
    "    print(\"Mistake breakdown:\")\n",
    "    for key, val in counter.most_common():\n",
    "        print(f\"{key:15}: {val}\")\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "analyze_errors(\"lyrics_test.iob2\", \"predictions_pop.iob2\")\n",
    "\n",
    "#------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f9609",
   "metadata": {},
   "source": [
    "## Confusion matrix of predicted entity labels\n",
    "\n",
    "- Rows = Gold (true) labels\n",
    "\n",
    "- Columns = Predicted labels\n",
    "\n",
    "Each cell counts how often a gold label was predicted as a certain label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab6d378-7e21-4c57-814b-6c81273fb6ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:05:58.876087Z",
     "iopub.status.busy": "2025-04-30T13:05:58.875354Z",
     "iopub.status.idle": "2025-04-30T13:06:01.855298Z",
     "shell.execute_reply": "2025-04-30T13:06:01.854344Z",
     "shell.execute_reply.started": "2025-04-30T13:05:58.876027Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_labels(file_path):\n",
    "    sentences = []\n",
    "    current = []\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if current:\n",
    "                    sentences.append(current)\n",
    "                    current = []\n",
    "            elif line.startswith(\"#\"):\n",
    "                continue\n",
    "            else:\n",
    "                current.append(line.split(\"\\t\")[2])\n",
    "    if current:\n",
    "        sentences.append(current)\n",
    "    return sentences\n",
    "\n",
    "def create_confusion_matrix(gold_path, pred_path):\n",
    "    gold_sentences = read_labels(gold_path)\n",
    "    pred_sentences = read_labels(pred_path)\n",
    "\n",
    "    assert len(gold_sentences) == len(pred_sentences), \"Mismatch in number of sentences\"\n",
    "\n",
    "    all_gold = []\n",
    "    all_pred = []\n",
    "\n",
    "    for g_sent, p_sent in zip(gold_sentences, pred_sentences):\n",
    "        assert len(g_sent) == len(p_sent), \"Mismatch in sentence lengths\"\n",
    "        all_gold.extend(g_sent)\n",
    "        all_pred.extend(p_sent)\n",
    "\n",
    "    #get all unique labels\n",
    "    all_labels = sorted(set(all_gold + all_pred))\n",
    "   \n",
    "    #build confusion matrix as DataFrame\n",
    "    confusion = pd.DataFrame(0, index=all_labels, columns=all_labels)\n",
    "\n",
    "    for g, p in zip(all_gold, all_pred):\n",
    "        confusion.loc[g, p] += 1\n",
    "\n",
    "    return confusion\n",
    "\n",
    "def plot_confusion_matrix(confusion_df, normalize=False, figsize=(12, 10)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    data = confusion_df.copy()\n",
    "    if normalize:\n",
    "        data = data.div(data.sum(axis=1), axis=0)\n",
    "    sns.heatmap(data, annot=True, fmt=\".2f\" if normalize else \"d\", cmap=\"Blues\")\n",
    "    plt.title(\"NER Confusion Matrix\" + (\" (Normalized)\" if normalize else \"\"))\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"Gold Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "gold_file = \"lyrics_test.iob2\"\n",
    "pred_file = \"predictions_pop.iob2\"\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "confusion = create_confusion_matrix(gold_file, pred_file)\n",
    "plot_confusion_matrix(confusion)             \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
